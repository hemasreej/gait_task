# -*- coding: utf-8 -*-
"""Abnormal_gait-video.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/110Ex4MsiDb2HtGHpi87vv3pDn5WONJZk
"""

from google.colab import drive
drive.mount('/content/drive')

rmdir /content/drive/MyDrive/train/.ipynb_checkpoints

import keras
from tensorflow.keras import layers

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

dataset_path=os.listdir('/content/drive/MyDrive/traingait')
print(dataset_path)

label_types=os.listdir('/content/drive/MyDrive/traingait')
print(label_types)

#rmdir /content/drive/MyDrive/train/.ipynb_checkpoints

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('/content/drive/MyDrive/traingait' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('/content/drive/MyDrive/traingait' + '/' +item) + '/' + room))

# Build a dataframe
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())

df=train_df.loc[:,['video_name','tag']]
df
df.to_csv('train.csv')

dataset_path=os.listdir('/content/drive/MyDrive/testgait')
print(dataset_path)

room_types=os.listdir('/content/drive/MyDrive/testgait')
print("Types of activities found:",len(dataset_path))

rooms=[]

for item in dataset_path:
  all_rooms=os.listdir('/content/drive/MyDrive/testgait'+'/'+item)

for item in all_rooms:
  rooms.append((item,str('/content/drive/MyDrive/testgait'+'/'+item)+'/'+room))

test_df=pd.DataFrame(data=rooms,columns=['tag','video_name'])
print(test_df)
#print(test_df.tail())

df=test_df.loc[:,['video_name','tag']]
df
df.to_csv('test.csv')

!pip install git+https://github.com/tensorflow/docs

from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])
  except RuntimeError as e:
    print(e)

train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

print(f"Total videos for training: {len(train_df)}")
print(f"Total videos for testing: {len(test_df)}")


train_df.sample(10)

IMG_SIZE=600

def crop_center_square(frame):
  y,x=frame.shape[0:2]
  min_dim=min(y,x)
  start_x=(x//2)-(min_dim//2)
  start_y=(y//2)-(min_dim//2)
  return frame[start_y : start_y+min_dim,start_x:start_x+min_dim]

def load_video(path,max_frames=0,resize=(IMG_SIZE,IMG_SIZE)):
  cap=cv2.VideoCapture(path)
  frames=[]
  try:
    while True:
      ret,frame=cap.read()
      if not ret:
        break
      frame =crop_center_square(frame)
      frame =cv2.resize(frame,resize)
      frame =frame[:,:,[2,1,0]]
      frames.append(frame)

      if len(frames)==max_frames:
        break
  finally:
    cap.release()
  return np.array(frames)

def build_feature_extractor():
    feature_extractor = keras.applications.InceptionV3(
        weights="imagenet",
        include_top=False,
        pooling="avg",
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
    )
    preprocess_input = keras.applications.inception_v3.preprocess_input

    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")


feature_extractor = build_feature_extractor()

label_processor = keras.layers.experimental.preprocessing.StringLookup(
    num_oov_indices=1, vocabulary=np.unique(train_df["tag"])
)
print(label_processor.get_vocabulary())

IMG_SIZE = 600
BATCH_SIZE = 64
EPOCHS = 10

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048

tf.keras.layers.StringLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token="[UNK]",
    vocabulary=None,
    idf_weights=None,
    encoding=None,
    invert=False,
    output_mode="int",
    sparse=False,
    pad_to_max_tokens=False,
    #**kwargs
)

def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()

    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values

    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "train")
test_data, test_labels = prepare_all_videos(test_df, "test")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

def get_sequence_model():
    class_vocab = label_processor.get_vocabulary()

    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")

    # Refer to the following tutorial to understand the significance of using `mask`:
    # https://keras.io/api/layers/recurrent_layers/gru/
    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)
    x = keras.layers.GRU(8)(x)
    x = keras.layers.Dropout(0.4)(x)
    x = keras.layers.Dense(8, activation="relu")(x)
    output = keras.layers.Dense(len(class_vocab), activation="softmax")(x)

    rnn_model = keras.Model([frame_features_input, mask_input], output)

    rnn_model.compile(
        loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"]
    )
    return rnn_model

EPOCHS = 30
# Utility for running experiments.
def run_experiment():
    filepath = "./tmp/video_classifier"
    checkpoint = keras.callbacks.ModelCheckpoint(
        filepath, save_weights_only=True, save_best_only=True, verbose=1
    )

    seq_model = get_sequence_model()
    history = seq_model.fit(
        [train_data[0], train_data[1]],
        train_labels,
        validation_split=0.3,
        epochs=EPOCHS,
        callbacks=[checkpoint],
    )

    seq_model.load_weights(filepath)
    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)
    #print(f"Test accuracy: {round(accuracy * 100, 2)}%")

    return history, seq_model


_, sequence_model = run_experiment()

# save the pre-trained model to a local directory
# define the file path for the saved model
model_file_path = "/content/drive/MyDrive/AG"

# train the model and save it to the file path
history, sequence_model = run_experiment()
sequence_model.save(model_file_path)

from tensorflow.keras.models import load_model

# load the saved model from the file path
model = load_model("/content/drive/MyDrive/AG")

def prepare_single_video(frames):
    frames = frames[None, ...]
    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")

    for i, batch in enumerate(frames):
        video_length = batch.shape[0]
        length = min(MAX_SEQ_LENGTH, video_length)
        for j in range(length):
            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])
        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

    return frame_features, frame_mask


def sequence_prediction(path):
    class_vocab = label_processor.get_vocabulary()

    frames = load_video(os.path.join("test", path))
    frame_features, frame_mask = prepare_single_video(frames)
    probabilities = sequence_model.predict([frame_features, frame_mask])[0]

    for i in np.argsort(probabilities)[::-1]:
        print(f"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%")
    return frames

test_video = np.random.choice(test_df["video_name"].values.tolist())
print(f"Test video path: {test_video}")

test_frames = sequence_prediction(test_video)

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip

from flask import Flask, render_template, request
from werkzeug.utils import secure_filename
import os

app = Flask(__name__)

# Load the trained model
model = load_model("/content/drive/MyDrive/AG")

# Define the allowed file extensions
ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'mkv'}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def predict_video(video_path):
    frames = load_video(video_path)
    frame_features, frame_mask = prepare_single_video(frames)
    probabilities = model.predict([frame_features, frame_mask])[0]

    class_vocab = label_processor.get_vocabulary()
    predictions = [{"class": class_vocab[i], "probability": float(probabilities[i])} for i in range(len(class_vocab))]

    return predictions

@app.route('/')
def index():
    return '''
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Video Classification</title>
    </head>
    <body>
        <h1>Video Classification</h1>

        <form method="post" enctype="multipart/form-data">
            <label for="file">Choose a video file:</label>
            <input type="file" id="file" name="file" accept=".mp4, .avi, .mov, .mkv" required>
            <br>
            <button type="submit">Submit</button>
        </form>
    </body>
    </html>
    '''

@app.route('/', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return 'No file part'

    file = request.files['file']

    if file.filename == '':
        return 'No selected file'

    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        file_path = os.path.join("/content/drive/MyDrive/uploads", filename)
        file.save(file_path)

        # Make predictions
        predictions = predict_video(file_path)

        return f'Predictions: {predictions}'

if __name__ == '__main__':
    # Install ngrok
    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
    !unzip ngrok-stable-linux-amd64.zip

    # Use ngrok to expose the app to the internet
    get_ipython().system_raw('./ngrok http 5000 &')

    # Start the Flask app
    app.run()

!pip install streamlit

# Save this code in a file named app.py
import streamlit as st

st.title("Streamlit App for Video Classification")

uploaded_file = st.file_uploader("Choose a video file", type=["mp4", "avi", "mov", "mkv"])

if uploaded_file:
    st.video(uploaded_file)

!streamlit run app.py

!pip install streamlit



model = load_model("/content/drive/MyDrive/AG")

"""USING STREAMLIT FOR DEPLOYMENT"""

!pip install streamlit

import streamlit as st
from tensorflow.keras.models import load_model
from PIL import Image
import io

# Load the model
model = load_model('/content/drive/MyDrive/AG')

# Define the classes
classes = ['normal', 'antalgic', 'steppage', 'lurching', 'trendelenburg']

def predict(file):
    # Read the image file
    image = Image.open(file)

    # Resize the image
    image = image.resize((224, 224))

    # Convert the image to a numpy array
    image_array = np.array(image)

    # Normalize the image
    image_array = image_array / 255.0

    # Add a batch dimension
    image_array = np.expand_dims(image_array, axis=0)

    # Predict the class
    predictions = model.predict(image_array)
    class_index = np.argmax(predictions)
    class_name = classes[class_index]

    return class_name

# Render the Streamlit app
st.title("Gait Classification")
file = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])
if file is not None:
    st.write("Class:", predict(file))

!streamlit run </usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py>

!touch app.py

!nano app.py

import cv2
import numpy as np
import streamlit as st
import tensorflow as tf

# Load the pre-trained deep learning model
model = tf.keras.models.load_model('/content/drive/MyDrive/AG')

# Define the gait classes
classes = ['normal', 'antalgic', 'steppage', 'lurching', 'trendelenburg', 'stifflegged']

# Define the function to preprocess the video data
def preprocess_video(video):
    # Preprocess the video data here
    return preprocessed_video

# Define the function to classify the video data
def classify_video(video):
    # Preprocess the video data
    preprocessed_video = preprocess_video(video)

    # Make predictions using the pre-trained model
    predictions = model.predict(preprocessed_video)

    # Get the predicted class
    predicted_class = classes[np.argmax(predictions)]

    return predicted_class

# Define the Streamlit app
def app():
    # Set the app title
    st.title('Gait Classification App')

    # Add a file uploader for the video input
    video_file = st.file_uploader('Upload a video file', type=['mp4'])

    # Check if a video file has been uploaded
    if video_file is not None:
        # Read the video data
        video_data = video_file.read()

        # Convert the video data to a numpy array
        video_data = np.fromstring(video_data, np.uint8)

        # Decode the video data
        video = cv2.imdecode(video_data, cv2.IMREAD_COLOR)

        # Classify the video data
        predicted_class = classify_video(video)

        # Display the classification result
        st.write('The predicted gait class is:', predicted_class)

# Run the Streamlit app
if __name__ == '__main__':
    app()

import streamlit as st

def classify_video(video):
    # Your code to classify the video goes here
    return "Gait classification"

uploaded_file = st.file_uploader("/content/drive/MyDrive/testgait/ataxic/1 (1).mp4")
if uploaded_file is not None:
    output = classify_video(uploaded_file)
    st.write(output)

import streamlit as st

def classify_video(video):
    print("Classifying video...")
    # Your code to classify the video goes here
    output = "Gait classification"
    print(f"Output: {output}")
    return output

uploaded_file = st.file_uploader("/content/drive/MyDrive/testgait/ataxic/1 (1).mp4")
if uploaded_file is not None:
    output = classify_video(uploaded_file)
    st.write(output)

import cv2
import numpy as np
import streamlit as st
import tensorflow as tf

# Load the pre-trained deep learning model
model = tf.keras.models.load_model('/content/drive/MyDrive/AG')

# Define the gait classes
classes = ['normal', 'antalgic', 'steppage', 'lurching', 'ttrendelenburg', 'stifflegged']

# Define the function to preprocess the video data
def preprocess_video(video):
    # Preprocess the video data here
    return preprocessed_video

# Define the function to classify the video data
def classify_video(video):
    # Preprocess the video data
    preprocessed_video = preprocess_video(video)

    # Make predictions using the pre-trained model
    predictions = model.predict(preprocessed_video)

    # Get the predicted class
    predicted_class = classes[np.argmax(predictions)]

    return predicted_class

# Define the Streamlit app
def app():
    # Set the app title
    st.title('Gait Classification App')

    # Add a file uploader for the video input
    video_file = st.file_uploader('Upload a video file', type=['mp4'])

    # Check if a video file has been uploaded
    if video_file is not None:
        # Read the video data
        video_data = video_file.read()

        # Convert the video data to a numpy array
        video_data = np.fromstring(video_data, np.uint8)

        # Decode the video data
        video = cv2.imdecode(video_data, cv2.IMREAD_COLOR)

        # Classify the video data
        predicted_class = classify_video(video)

        # Display the classification result
        st.write('The predicted gait class is:', predicted_class)

# Run the Streamlit app
if __name__ == '__main__':
    app()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import cv2
# 
# def classify_gait(video_path):
#     cap = cv2.VideoCapture(video_path)
#     while True:
#         ret, frame = cap.read()
#         if not ret:
#             break
#         # Your code to classify each frame goes here
#     cap.release()
#     return "Normal gait"
# 
# def main():
#     st.title("Gait Classification App")
#     uploaded_file = st.file_uploader("Upload a video", type=["mp4"])
#     if uploaded_file is not None:
#         video_path = "uploaded_video.mp4"
#         with open(video_path, "wb") as f:
#             f.write(uploaded_file.getbuffer())
#         gait_type = classify_gait(video_path)
#         st.write("Gait type:", gait_type)
# 
# if __name__ == "__main__":
#     main()

!choco install ngrok

!ngrok config add-authtoken 2Z4Rg3IQNVkVuADyB68ltgr82uv_4ATc6TmUXgddQYvztkBTD

#!npm install -g localtunnel

!streamlit run app.py
!ngrok http 80

!npm install -g localtunnel

!streamlit run app.py &>/dev/null&
!lt --port 8501

pip install gradio

# Install necessary libraries
!pip install gradio
!pip install tensorflow
!pip install pyngrok

# Import libraries
import gradio as gr
from tensorflow.keras.models import load_model
from PIL import Image
import numpy as np
from pyngrok import ngrok

# Load the model
model = load_model('/content/drive/MyDrive/AG')

# Define the classes
classes = ['normal', 'antalgic', 'steppage', 'lurching', 'trendelenburg']

# Define the predict function
def predict_image(file):
    image = Image.open(file.name)
    image = image.resize((224, 224))
    image_array = np.array(image)
    image_array = image_array / 255.0
    image_array = np.expand_dims(image_array, axis=0)
    predictions = model.predict(image_array)
    class_index = np.argmax(predictions)
    class_name = classes[class_index]
    return class_name

# Create a simple UI with Gradio
iface = gr.Interface(fn=predict_image, inputs="image", outputs="text")

# Launch ngrok and create a public URL
public_url = ngrok.connect(port=iface.port)

# Launch the Gradio interface
iface.launch(share=True)

# Print the public URL
print(f'Open the interface at: {public_url}')

# Install necessary libraries
!pip install flask-ngrok

# Import libraries
from flask import Flask, request
from flask_ngrok import run_with_ngrok
from tensorflow.keras.models import load_model
from PIL import Image
import numpy as np

# Load the model
model = load_model('/content/drive/MyDrive/AG')

# Define the classes
classes = ['normal', 'antalgic', 'steppage', 'lurching', 'trendelenburg']

# Create Flask app
app = Flask(__name__)
run_with_ngrok(app)  # Start ngrok when app is run

# Define the predict function
def predict_image(file):
    image = Image.open(file.filename)
    image = image.resize((224, 224))
    image_array = np.array(image)
    image_array = image_array / 255.0
    image_array = np.expand_dims(image_array, axis=0)
    predictions = model.predict(image_array)
    class_index = np.argmax(predictions)
    class_name = classes[class_index]
    return class_name

# Define the route for prediction
@app.route('/predict', methods=['POST'])
def predict():
    file = request.files['file']
    result = predict_image(file)
    return result

# Run the Flask app
if __name__ == '__main__':
    app.run()

!pip install gradio

import gradio as gr
from tensorflow.keras.models import load_model
from PIL import Image
import numpy as np

# Load the model
model = load_model('/content/drive/MyDrive/AG')

# Define the classes
classes = ['normal', 'antalgic', 'steppage', 'lurching', 'trendelenburg']

# Define the predict function
def predict_image(file):
    image = Image.open(file.name)
    image = image.resize((224, 224))
    image_array = np.array(image)
    image_array = image_array / 255.0
    image_array = np.expand_dims(image_array, axis=0)
    predictions = model.predict(image_array)
    class_index = np.argmax(predictions)
    class_name = classes[class_index]
    return class_name

# Create a Gradio interface
iface = gr.Interface(fn=predict_image, inputs="image", outputs="text")

# Launch the Gradio interface
iface.launch()